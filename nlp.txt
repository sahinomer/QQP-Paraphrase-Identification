
Train on 327474 samples, validate on 36387 samples
Train on 327474 samples, validate on 36387 samples


siamese

 merged = concatenate([dot_questions, subtract_q1q2, subtract_q2q1, multiply_questions])
   self.model.compile(optimizer=Adam(lr=1e-3),
                           loss='binary_crossentropy',
                           metrics=['binary_crossentropy', 'binary_accuracy'])

327474/327474 [==============================] - 1167s 4ms/step - loss: 0.2330 - binary_crossentropy: 0.2330 - binary_accuracy: 0.9022 - val_loss: 0.3521 - val_binary_crossentropy: 0.3521 - val_binary_accuracy: 0.8480

Test
[loss:0.3477534324696641, binary_crossentropy:0.3477534324696641, binary_accuracy:0.8462984491433682]

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

similarity = Lambda(euclidean_similarity,
                    output_shape=euclidean_output_shape)([question1_lstm, question2_lstm])

out = Dense(1, activation='sigmoid')(similarity)

self.model.compile(optimizer=Adam(lr=1e-3),
                           loss='mean_squared_error',
                           metrics=['binary_crossentropy', 'binary_accuracy'])
327474/327474 [==============================] - 1195s 4ms/step - loss: 0.1488 - binary_crossentropy: 0.4634 - binary_accuracy: 0.7907 - val_loss: 0.1749 - val_binary_crossentropy: 0.5298 - val_binary_accuracy: 0.7498
[mse:0.17276270731035226, binary_crossentropy:0.5244555308161336, binary_accuracy:z0.7535927181205093]


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


dot_questions = dot([question1_lstm, question2_lstm], axes=1, normalize=True)

out = Dense(1, activation='sigmoid')(dot_questions)

self.model.compile(optimizer=Adam(lr=1e-3),
                    loss='mean_squared_error',
                    metrics=['binary_crossentropy', 'binary_accuracy'])

327474/327474 [==============================] - 1225s 4ms/step - loss: 0.0844 - binary_crossentropy: 0.2852 - binary_accuracy: 0.8893 - val_loss: 0.1196 - val_binary_crossentropy: 0.3823 - val_binary_accuracy: 0.8347
[mse:0.11937432466526561, binary_crossentropy:0.3813874629235656, binary_accuracy:0.8330406391569625]


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Soft Attention Mechanism
C:\PycharmProjects\QQP-Paraphrase-Identification\models\siamese_attention_dnn_2019-12-22

327474/327474 [==============================] - 2011s 6ms/step - loss: 0.2086 - binary_crossentropy: 0.2086 - binary_accuracy: 0.9144 - val_loss: 0.3149 - val_binary_crossentropy: 0.3149 - val_binary_accuracy: 0.8687
[0.3121551997603385, 0.3121551997603385, 0.8689307180577711]


+weights
327474/327474 [==============================] - 1992s 6ms/step - loss: 0.2133 - binary_crossentropy: 0.2133 - binary_accuracy: 0.9117 - val_loss: 0.3169 - val_binary_crossentropy: 0.3169 - val_binary_accuracy: 0.8671
[0.31126270462864597, 0.31126270462864597, 0.8673971654143422]

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

soft atttention + preprocess
327474/327474 [==============================] - 1876s 6ms/step - loss: 0.2409 - binary_crossentropy: 0.2409 - binary_accuracy: 0.8981 - val_loss: 0.3248 - val_binary_crossentropy: 0.3248 - val_binary_accuracy: 0.8600
[0.3209092228076605, 0.3209092228076605, 0.8612382200998598]


soft atttention + preprocess++
327474/327474 [==============================] - 1879s 6ms/step - loss: 0.2405 - binary_crossentropy: 0.2405 - binary_accuracy: 0.8982 - val_loss: 0.3144 - val_binary_crossentropy: 0.3144 - val_binary_accuracy: 0.8625
[0.30946718860797146, 0.30946718860797146, 0.8638106309974535]


soft attention + preprocess++ + weights class
327474/327474 [==============================] - 1883s 6ms/step - loss: 0.2459 - binary_crossentropy: 0.2459 - binary_accuracy: 0.8952 - val_loss: 0.3188 - val_binary_crossentropy: 0.3188 - val_binary_accuracy: 0.8619
[0.31588850711130273, 0.31588850711130273, 0.8621781394631979]

--------------------------------------------------------------------------------------------------------------------------

soft attention + preprocess++ + 500d word embedding
327474/327474 [==============================] - 1972s 6ms/step - loss: 0.2618 - binary_crossentropy: 0.2618 - binary_accuracy: 0.8878 - val_loss: 0.3154 - val_binary_crossentropy: 0.3154 - val_binary_accuracy: 0.8625
[0.31523596060193765, 0.31523596060193765, 0.8607435256958527]

siamese_attention_dnn+8epoch+%87_2020-01-21
soft attention + preprocess++ + 8 epoch
327474/327474 [==============================] - 1883s 6ms/step - loss: 0.1776 - binary_crossentropy: 0.1776 - binary_accuracy: 0.9276 - val_loss: 0.3313 - val_binary_crossentropy: 0.3313 - val_binary_accuracy: 0.8714
[0.33092244544382243, 0.33092244544382243, 0.8717010067119549]

---------------------------------------------------------------------------------------------------------------------------

word mover distance
(1.5640443442692298, 0.6643993173217245)

wmd + preprocess
(1.2281292357982587, 0.6594523732963962)

